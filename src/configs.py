episodes_3x3 = 30
max_steps_3x3 = 8
ph_3x3 = 7
episodes_4x4 = 50
max_steps_4x4 = 15
ph_4x4 = 10
episodes_6x4 = 150
max_steps_6x4 = 25
ph_6x4 = 18
episodes_7x4 = 175
max_steps_7x4 = 30
ph_7x4 = 20
episodes_8x8 = 400
max_steps_8x8 = 60
ph_8x8 = 40


configs = {

    # "T0": {"repetitions": 3, "episodes": 20, "max_steps": 30,
    #        "frozenlake": {"name": "FrozenLake4x4_A", "traverser_path": None, "slippery": True},
    #        "learning": {"norm_set": 7, "epsilon": 0.3, "initialisation": "zero | random | distance | safe | state_function | state_action_penalty", "reversed_q_learning": True, "discount": 0.95, "learning_rate": 0.6, "learning_rate_strategy": "constant", "exponential_decay", "linear_decay", "learning_decay_rate": 0.02},
    #        "planning": {"norm_set": 7, "delta": 0.5, "strategy": "no_planning | full_planning | plan_for_new_states | delta_greedy_planning | delta_decaying_planning", "planning_horizon": 14, "reward_set" : 1},
    #        "deontic": {"norm_set": 8, "evaluation_function": 3},
    #        "enforcing": {"norm_set": 6, "strategy": "guardrail | fixing | optimal_reward_shaping | full_reward_shaping", "phase": "during_training | after_training", "enforcing_horizon": [3,6] (no use in guardral; in fixing is list [len of checked path; len of fixed path]; in reward-shaping defines number of shaping steps)},
    #        },

    "T1": {"repetitions": 2, "episodes": 30, "max_steps": 70, "evaluation_repetitions": 20,
               "frozenlake": {"name": "FrozenLake8x8_A", "traverser_path": "8x8_A", "slippery": True},
               "learning": {"norm_set": None, "epsilon": 0.3, "initialisation": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
               "planning": {"norm_set": 1, "delta": 0.9, "strategy": "delta_greedy_planning", "planning_horizon": 35, "reward_set": 2},
               "deontic": {"norm_set": 0, "evaluation_function": 4},
               "enforcing": None,
               },

    "T2": {"repetitions": 3, "episodes": 30, "max_steps": 25, "evaluation_repetitions": 20,
               "frozenlake": {"name": "FrozenLake6x4_A", "traverser_path": "6x4_A", "slippery": True},
               "learning": {"norm_set": None, "epsilon": 0.3, "initialisation": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
               "planning": {"norm_set": 1, "delta": 0.6, "strategy": "delta_greedy_planning", "planning_horizon": 20, "reward_set": 2},
               "deontic": {"norm_set": 0, "evaluation_function": 4},
               "enforcing": None,
               },

    "T3": {"repetitions": 1, "episodes": 60, "max_steps": 15, "evaluation_repetitions": 20,
               "frozenlake": {"name": "FrozenLake3x3_A", "traverser_path": "3x3_A", "slippery": True},
               "learning": {"norm_set": None, "epsilon": 0.3, "initialisation": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": 0.02},
               "planning": {"norm_set": 3, "delta": 0.8, "strategy": "delta_greedy_planning", "planning_horizon": 8, "reward_set": 2},
               "deontic": {"norm_set": 0, "evaluation_function": 4},
               },

    "ww4": {"repetitions": 3, "episodes": 15, "max_steps": 40,
           "frozenlake": {"name": "FrozenLake4x4_A", "traverser_path": "4x4_A", "slippery": False},
           "learning": {"epsilon": 0.3, "initialisation": "zero", "reversed_q_learning": True, "discount": 0.95, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"delta": 0.75, "strategy": "delta_greedy_planning", "planning_horizon": 12},
           "deontic": {"norm_set": 3, "evaluation_function": 3},
           "enforcing": {"norm_set": 6, "strategy": "full_reward_shaping", "phase": "after_training", "enforcing_horizon": 4},
           },



    # A* to test RL-params
    # levels without norms, just structure: 3x3_A, 4x4_A, 6x4_A, 6x4_B, 7x4_A, 7x4_C, 8x8_A

    "A1": {"repetitions": 100, "episodes": episodes_3x3, "max_steps": max_steps_3x3, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake3x3_A", "traverser_path": "3x3_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.15, "initialisation": "distance", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "no_planning", "planning_horizon": None, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": None},
           "enforcing": None,
           },

    "A2": {"repetitions": 100, "episodes": episodes_4x4, "max_steps": max_steps_4x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake4x4_A", "traverser_path": "4x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.15, "initialisation": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "no_planning", "planning_horizon": None, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": None},
           "enforcing": None,
           },

    "A3": {"repetitions": 100, "episodes": episodes_4x4, "max_steps": max_steps_4x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake4x4_A", "traverser_path": "4x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.15, "initialisation": "distance", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "no_planning", "planning_horizon": None, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": None},
           "enforcing": None,
           },

    "A4": {"repetitions": 100, "episodes": episodes_6x4, "max_steps": max_steps_6x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake6x4_A", "traverser_path": "6x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.15, "initialisation": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "no_planning", "planning_horizon": None, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": None},
           "enforcing": None,
           },

    "A5": {"repetitions": 100, "episodes": episodes_6x4, "max_steps": max_steps_6x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake6x4_B", "traverser_path": "6x4_B", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.15, "initialisation": "distance", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "no_planning", "planning_horizon": None, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": None},
           "enforcing": None,
           },

    "A6": {"repetitions": 100, "episodes": episodes_7x4, "max_steps": max_steps_7x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake7x4_A", "traverser_path": "7x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.15, "initialisation": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "no_planning", "planning_horizon": None, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": None},
           "enforcing": None,
           },

    "A7": {"repetitions": 100, "episodes": episodes_7x4, "max_steps": max_steps_7x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake7x4_C", "traverser_path": "7x4_C", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.15, "initialisation": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "no_planning", "planning_horizon": None, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": None},
           "enforcing": None,
           },

    "A8": {"repetitions": 100, "episodes": episodes_8x8, "max_steps": max_steps_8x8, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake8x8_A", "traverser_path": "8x8_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.15, "initialisation": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "no_planning", "planning_horizon": None, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": None},
           "enforcing": None,
           },


    # B* to test policy strategies
    # TODO: take the same levels as before and test with one planning strategy for them, ie compare benefit of planning to them
    #  strategy: greedy or first visits ? or greedy+visit+full and make then three variants for each level? (latter better for comparisons, also first visit might then become the best)
    #  increase epsilon to higher values  ..
    # "planning": {"norm_set": 1, "delta": 0.x, "strategy": "delta_greedy_planning", "planning_horizon": x, "reward_set": 2},
    "B1_newstates": {"repetitions": 100, "episodes": episodes_3x3, "max_steps": max_steps_3x3, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake3x3_A", "traverser_path": "3x3_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.15, "initialisation": "distance", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_3x3, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },

    "B2_greedy": {"repetitions": 100, "episodes": episodes_4x4, "max_steps": max_steps_4x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake4x4_A", "traverser_path": "4x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.15, "initialisation": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": 0.5, "strategy": "delta_greedy_planning", "planning_horizon": ph_4x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B2_decay": {"repetitions": 100, "episodes": episodes_4x4, "max_steps": max_steps_4x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake4x4_A", "traverser_path": "4x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.15, "initialisation": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": 0.00005, "strategy": "delta_decaying_planning", "planning_horizon": ph_4x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B2_newstates": {"repetitions": 100, "episodes": episodes_4x4, "max_steps": max_steps_4x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake4x4_A", "traverser_path": "4x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.15, "initialisation": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_4x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B2_full": {"repetitions": 100, "episodes": episodes_4x4, "max_steps": max_steps_4x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake4x4_A", "traverser_path": "4x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.15, "initialisation": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "full_planning", "planning_horizon": ph_4x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },

    "B3_greedy": {"repetitions": 100, "episodes": episodes_4x4, "max_steps": max_steps_4x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake4x4_A", "traverser_path": "4x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.15, "initialisation": "distance", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": 0.5, "strategy": "delta_greedy_planning", "planning_horizon": ph_4x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B3_decay": {"repetitions": 100, "episodes": episodes_4x4, "max_steps": max_steps_4x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake4x4_A", "traverser_path": "4x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.15, "initialisation": "distance", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": 0.00005, "strategy": "delta_decaying_planning", "planning_horizon": ph_4x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B3_newstates": {"repetitions": 100, "episodes": episodes_4x4, "max_steps": max_steps_4x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake4x4_A", "traverser_path": "4x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.15, "initialisation": "distance", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_4x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B3_full": {"repetitions": 100, "episodes": episodes_4x4, "max_steps": max_steps_4x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake4x4_A", "traverser_path": "4x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.15, "initialisation": "distance", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "full_planning", "planning_horizon": ph_4x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },

    "B4_greedy": {"repetitions": 100, "episodes": episodes_6x4, "max_steps": max_steps_6x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake6x4_A", "traverser_path": "6x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.15, "initialisation": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": 0.5, "strategy": "delta_greedy_planning", "planning_horizon": ph_6x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B4_decay": {"repetitions": 100, "episodes": episodes_6x4, "max_steps": max_steps_6x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake6x4_A", "traverser_path": "6x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.15, "initialisation": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": 0.00005, "strategy": "delta_decaying_planning", "planning_horizon": ph_6x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B4_newstates": {"repetitions": 100, "episodes": episodes_6x4, "max_steps": max_steps_6x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake6x4_A", "traverser_path": "6x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.15, "initialisation": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_6x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B4_full": {"repetitions": 100, "episodes": episodes_6x4, "max_steps": max_steps_6x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake6x4_A", "traverser_path": "6x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.15, "initialisation": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "full_planning", "planning_horizon": ph_6x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },

    "B5_greedy": {"repetitions": 100, "episodes": episodes_6x4, "max_steps": max_steps_6x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake6x4_B", "traverser_path": "6x4_B", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.15, "initialisation": "distance", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": 0.5, "strategy": "delta_greedy_planning", "planning_horizon": ph_6x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B5_decay": {"repetitions": 100, "episodes": episodes_6x4, "max_steps": max_steps_6x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake6x4_B", "traverser_path": "6x4_B", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.15, "initialisation": "distance", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": 0.00005, "strategy": "delta_decaying_planning", "planning_horizon": ph_6x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B5_newstates": {"repetitions": 100, "episodes": episodes_6x4, "max_steps": max_steps_6x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake6x4_B", "traverser_path": "6x4_B", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.15, "initialisation": "distance", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_6x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B5_full": {"repetitions": 100, "episodes": episodes_6x4, "max_steps": max_steps_6x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake6x4_B", "traverser_path": "6x4_B", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.15, "initialisation": "distance", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "full_planning", "planning_horizon": ph_6x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },

    "B6_greedy": {"repetitions": 100, "episodes": episodes_7x4, "max_steps": max_steps_7x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake7x4_A", "traverser_path": "7x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.15, "initialisation": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": 0.5, "strategy": "delta_greedy_planning", "planning_horizon": ph_7x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B6_decay": {"repetitions": 100, "episodes": episodes_7x4, "max_steps": max_steps_7x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake7x4_A", "traverser_path": "7x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.15, "initialisation": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": 0.00005, "strategy": "delta_decaying_planning", "planning_horizon": ph_7x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B6_newstates": {"repetitions": 100, "episodes": episodes_7x4, "max_steps": max_steps_7x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake7x4_A", "traverser_path": "7x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.15, "initialisation": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_7x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           }, # TODO: redo the B's before this; maybe increase the delta from decay as well?
    "B6_full": {"repetitions": 100, "episodes": episodes_7x4, "max_steps": max_steps_7x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake7x4_A", "traverser_path": "7x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.15, "initialisation": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "full_planning", "planning_horizon": ph_7x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },

    "B7_greedy": {"repetitions": 100, "episodes": episodes_7x4, "max_steps": max_steps_7x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake7x4_C", "traverser_path": "7x4_C", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.15, "initialisation": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": 0.5, "strategy": "delta_greedy_planning", "planning_horizon": ph_7x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B7_decay": {"repetitions": 100, "episodes": episodes_7x4, "max_steps": max_steps_7x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake7x4_C", "traverser_path": "7x4_C", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.25, "initialisation": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": 0.000005, "strategy": "delta_decaying_planning", "planning_horizon": ph_7x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B7_newstates": {"repetitions": 100, "episodes": episodes_7x4, "max_steps": max_steps_7x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake7x4_C", "traverser_path": "7x4_C", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.15, "initialisation": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_7x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B7_full": {"repetitions": 100, "episodes": episodes_7x4, "max_steps": max_steps_7x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake7x4_C", "traverser_path": "7x4_C", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.15, "initialisation": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "full_planning", "planning_horizon": ph_7x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },

    "B8_greedy": {"repetitions": 100, "episodes": episodes_8x8, "max_steps": max_steps_8x8, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake8x8_A", "traverser_path": "8x8_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.15, "initialisation": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": 0.5, "strategy": "delta_greedy_planning", "planning_horizon": ph_8x8, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B8_decay": {"repetitions": 100, "episodes": episodes_8x8, "max_steps": max_steps_8x8, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake8x8_A", "traverser_path": "8x8_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.15, "initialisation": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": 0.00005, "strategy": "delta_decaying_planning", "planning_horizon": ph_8x8, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B8_newstates": {"repetitions": 100, "episodes": episodes_8x8, "max_steps": max_steps_8x8, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake8x8_A", "traverser_path": "8x8_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.15, "initialisation": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_8x8, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B8_full": {"repetitions": 100, "episodes": episodes_8x8, "max_steps": max_steps_8x8, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake8x8_A", "traverser_path": "8x8_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.15, "initialisation": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "full_planning", "planning_horizon": ph_8x8, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },

    "B9_greedy": {"repetitions": 100, "episodes": episodes_8x8, "max_steps": max_steps_8x8, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake8x8_A", "traverser_path": "8x8_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.15, "initialisation": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": 0.5, "strategy": "delta_greedy_planning", "planning_horizon": ph_8x8, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B9_decay": {"repetitions": 100, "episodes": episodes_8x8, "max_steps": max_steps_8x8, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake8x8_A", "traverser_path": "8x8_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.15, "initialisation": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": 0.00005, "strategy": "delta_decaying_planning", "planning_horizon": ph_8x8, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B9_newstates": {"repetitions": 100, "episodes": episodes_8x8, "max_steps": max_steps_8x8, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake8x8_A", "traverser_path": "8x8_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.15, "initialisation": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_8x8, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B9_full": {"repetitions": 100, "episodes": episodes_8x8, "max_steps": max_steps_8x8, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake8x8_A", "traverser_path": "8x8_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.15, "initialisation": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "full_planning", "planning_horizon": ph_8x8, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },

    # TODO: Notes from B*
    #  results for high early planning (full,new,decay) ignore the initialization
    #  all plannings outperformed no_plannings, and learned the shortest path to goal
    #  higher planning yields better performance
    #  decay offers early satisfaction, but it's rate might be too fast?
    #  exploration underperformed, maybe epsilon was too low, or needs to be different to the planning strat? could also argue that concret epsilon strategies would help
    #  maybe for the decay use exploration later on?
    #  for B6, the planning component aimed for the shortest path, but after the traverser blocks it, it redirects the agent, this extra step is also learned into the target policy
    #  B7 the right path was learned, but the rewards are low, due to narrow path and risk of sliding (An edge kissing was not learned, since the model has not integrated this)
    #  B7 was the hardest level (7x4_C)
    #  .
    #  .......

# TODO: redo B7 with different epsilons/decays and extend the one plot again


    # C* to test norms and CTD
    # TODO: here planning strategy=no_planning;plan_new_states;full_planning?
    #  but only... some levels should have all planning strategies and later on maybe just planning for new states?
    # TODO: deontic aspects should be simple norms and then with ctds, also some larger ones with different priorities
    # TODO: foreach config here define hypotheses




    "U4_1": {"repetitions": 20, "episodes": 60, "max_steps": 20, "evaluation_repetitions": 20,
           "frozenlake": {"name": "FrozenLake4x4_B", "traverser_path": "4x4_B", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.3, "initialisation": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": 0.02},
           "planning": {"norm_set": 8, "delta": 0.5, "strategy": "delta_greedy_planning", "planning_horizon": 9, "reward_set" : 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": {"norm_set": 10, "strategy": "guardrail", "phase": "after_training", "enforcing_horizon": None},
           },
    "U6_1": {"repetitions": 20, "episodes": 100, "max_steps": 30,
           "frozenlake": {"name": "FrozenLake6x4_A", "traverser_path": "6x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.3, "initialisation": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": 0.02},
           "planning": {"norm_set": 9, "delta": 0.75, "strategy": "delta_greedy_planning", "planning_horizon": 14, "reward_set" : 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": {"norm_set": 11, "strategy": "guardrail", "phase": "after_training", "enforcing_horizon": None},
           },

    "U4_2": {"repetitions": 20, "episodes": 60, "max_steps": 20, "evaluation_repetitions": 20,
           "frozenlake": {"name": "FrozenLake4x4_B", "traverser_path": "4x4_B", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.3, "initialisation": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": 0.02},
           "planning": {"norm_set": 8, "delta": 0.5, "strategy": "delta_greedy_planning", "planning_horizon": 9, "reward_set" : 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": {"norm_set": 10, "strategy": "fixing", "phase": "after_training", "enforcing_horizon": [4,9]},
           },
    "U6_2": {"repetitions": 20, "episodes": 100, "max_steps": 30,
           "frozenlake": {"name": "FrozenLake6x4_A", "traverser_path": "6x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.3, "initialisation": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": 0.02},
           "planning": {"norm_set": 9, "delta": 0.75, "strategy": "delta_greedy_planning", "planning_horizon": 14, "reward_set" : 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": {"norm_set": 11, "strategy": "fixing", "phase": "after_training", "enforcing_horizon": [7,14]},
           },

    "U4_3": {"repetitions": 20, "episodes": 60, "max_steps": 20, "evaluation_repetitions": 20,
           "frozenlake": {"name": "FrozenLake4x4_B", "traverser_path": "4x4_B", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.3, "initialisation": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": 0.02},
           "planning": {"norm_set": 8, "delta": 0.5, "strategy": "delta_greedy_planning", "planning_horizon": 9, "reward_set" : 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": {"norm_set": 10, "strategy": "optimal_reward_shaping", "phase": "after_training", "enforcing_horizon": [60]},
           },
    "U6_3": {"repetitions": 20, "episodes": 100, "max_steps": 30,
           "frozenlake": {"name": "FrozenLake6x4_A", "traverser_path": "6x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.3, "initialisation": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": 0.02},
           "planning": {"norm_set": 9, "delta": 0.75, "strategy": "delta_greedy_planning", "planning_horizon": 14, "reward_set" : 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": {"norm_set": 11, "strategy": "optimal_reward_shaping", "phase": "after_training", "enforcing_horizon": [75]},
           },


    "U4_4": {"repetitions": 20, "episodes": 60, "max_steps": 20, "evaluation_repetitions": 20,
           "frozenlake": {"name": "FrozenLake4x4_B", "traverser_path": "4x4_B", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.3, "initialisation": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": 0.02},
           "planning": {"norm_set": 8, "delta": 0.5, "strategy": "delta_greedy_planning", "planning_horizon": 9, "reward_set" : 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": {"norm_set": 10, "strategy": "full_reward_shaping", "phase": "after_training", "enforcing_horizon": [60]},
           },
    "U6_4": {"repetitions": 20, "episodes": 100, "max_steps": 30,
           "frozenlake": {"name": "FrozenLake6x4_A", "traverser_path": "6x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.3, "initialisation": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": 0.02},
           "planning": {"norm_set": 9, "delta": 0.75, "strategy": "delta_greedy_planning", "planning_horizon": 14, "reward_set" : 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": {"norm_set": 11, "strategy": "full_reward_shaping", "phase": "after_training", "enforcing_horizon": [75]},
           },

    "U4_5": {"repetitions": 20, "episodes": 60, "max_steps": 20, "evaluation_repetitions": 20,
           "frozenlake": {"name": "FrozenLake4x4_B", "traverser_path": "4x4_B", "slippery": True},
           "learning": {"norm_set": 10, "epsilon": 0.3, "initialisation": "state_function", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": 0.02},
           "planning": {"norm_set": 8, "delta": 0.5, "strategy": "delta_greedy_planning", "planning_horizon": 9, "reward_set" : 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "U6_5": {"repetitions": 20, "episodes": 100, "max_steps": 30,
           "frozenlake": {"name": "FrozenLake6x4_A", "traverser_path": "6x4_A", "slippery": True},
           "learning": {"norm_set": 11, "epsilon": 0.3, "initialisation": "state_function", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": 0.02},
           "planning": {"norm_set": 9, "delta": 0.75, "strategy": "delta_greedy_planning", "planning_horizon": 14, "reward_set" : 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },

    "U4_6": {"repetitions": 20, "episodes": 60, "max_steps": 20, "evaluation_repetitions": 20,
           "frozenlake": {"name": "FrozenLake4x4_B", "traverser_path": "4x4_B", "slippery": True},
           "learning": {"norm_set": 10, "epsilon": 0.3, "initialisation": "state_action_penalty", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": 0.02},
           "planning": {"norm_set": 8, "delta": 0.5, "strategy": "delta_greedy_planning", "planning_horizon": 9, "reward_set" : 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "U6_6": {"repetitions": 20, "episodes": 100, "max_steps": 30,
           "frozenlake": {"name": "FrozenLake6x4_A", "traverser_path": "6x4_A", "slippery": True},
           "learning": {"norm_set": 11, "epsilon": 0.3, "initialisation": "state_action_penalty", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": 0.02},
           "planning": {"norm_set": 9, "delta": 0.75, "strategy": "delta_greedy_planning", "planning_horizon": 14, "reward_set" : 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "U4_7": {"repetitions": 20, "episodes": 60, "max_steps": 20, "evaluation_repetitions": 20,
           "frozenlake": {"name": "FrozenLake4x4_B", "traverser_path": "4x4_B", "slippery": True},
           "learning": {"norm_set": 10, "epsilon": 0.3, "initialisation": "distance", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": 0.02},
           "planning": {"norm_set": 8, "delta": 0.5, "strategy": "delta_greedy_planning", "planning_horizon": 9, "reward_set" : 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "U6_7": {"repetitions": 20, "episodes": 100, "max_steps": 30,
           "frozenlake": {"name": "FrozenLake6x4_A", "traverser_path": "6x4_A", "slippery": True},
           "learning": {"norm_set": 11, "epsilon": 0.3, "initialisation": "distance", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": 0.02},
           "planning": {"norm_set": 9, "delta": 0.75, "strategy": "delta_greedy_planning", "planning_horizon": 14, "reward_set" : 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
}