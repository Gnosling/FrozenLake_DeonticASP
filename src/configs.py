episodes_3x3 = 30
max_steps_3x3 = 8
ph_3x3 = 7
episodes_4x4 = 50
max_steps_4x4 = 15
ph_4x4 = 10
episodes_6x4 = 150
max_steps_6x4 = 25
ph_6x4 = 18
episodes_7x4 = 175
max_steps_7x4 = 30
ph_7x4 = 20
episodes_8x8 = 400
max_steps_8x8 = 60
ph_8x8 = 40

epsilon = 0.15


configs = {

    # "T0": {"repetitions": 3, "episodes": 20, "max_steps": 30,
    #        "frozenlake": {"name": "FrozenLake4x4_A", "traverser_path": None, "slippery": True},
    #        "learning": {"norm_set": 7, "epsilon": 0.3, "initialization": "zero | random | distance | safe | state_function | state_action_penalty", "reversed_q_learning": True, "discount": 0.95, "learning_rate": 0.6, "learning_rate_strategy": "constant", "exponential_decay", "linear_decay", "learning_decay_rate": 0.02},
    #        "planning": {"norm_set": 7, "delta": 0.5, "strategy": "no_planning | full_planning | plan_for_new_states | delta_greedy_planning | delta_decaying_planning", "planning_horizon": 14, "reward_set" : 1},
    #        "deontic": {"norm_set": 8, "evaluation_function": 3},
    #        "enforcing": {"norm_set": 6, "strategy": "guardrail | fixing | optimal_reward_shaping | full_reward_shaping", "phase": "during_training | after_training", "enforcing_horizon": [3,6] (no use in guardral; in fixing is list [len of checked path; len of fixed path]; in reward-shaping defines number of shaping steps)},
    #        },

    "T1": {"repetitions": 2, "episodes": 30, "max_steps": 70, "evaluation_repetitions": 20,
               "frozenlake": {"name": "FrozenLake8x8_A", "traverser_path": "8x8_A", "slippery": True},
               "learning": {"norm_set": None, "epsilon": 0.3, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
               "planning": {"norm_set": 1, "delta": 0.9, "strategy": "delta_greedy_planning", "planning_horizon": 35, "reward_set": 2},
               "deontic": {"norm_set": 0, "evaluation_function": 4},
               "enforcing": None,
               },

    "T2": {"repetitions": 3, "episodes": 30, "max_steps": 25, "evaluation_repetitions": 20,
               "frozenlake": {"name": "FrozenLake6x4_A", "traverser_path": "6x4_A", "slippery": True},
               "learning": {"norm_set": None, "epsilon": 0.3, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
               "planning": {"norm_set": 1, "delta": 0.6, "strategy": "delta_greedy_planning", "planning_horizon": 20, "reward_set": 2},
               "deontic": {"norm_set": 0, "evaluation_function": 4},
               "enforcing": None,
               },

    "T3": {"repetitions": 1, "episodes": 60, "max_steps": 15, "evaluation_repetitions": 20,
               "frozenlake": {"name": "FrozenLake3x3_A", "traverser_path": "3x3_A", "slippery": True},
               "learning": {"norm_set": None, "epsilon": 0.3, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": 0.02},
               "planning": {"norm_set": 3, "delta": 0.8, "strategy": "delta_greedy_planning", "planning_horizon": 8, "reward_set": 2},
               "deontic": {"norm_set": 0, "evaluation_function": 4},
               },

    "ww4": {"repetitions": 3, "episodes": 15, "max_steps": 40,
           "frozenlake": {"name": "FrozenLake4x4_A", "traverser_path": "4x4_A", "slippery": False},
           "learning": {"epsilon": 0.3, "initialization": "zero", "reversed_q_learning": True, "discount": 0.95, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"delta": 0.75, "strategy": "delta_greedy_planning", "planning_horizon": 12},
           "deontic": {"norm_set": 3, "evaluation_function": 3},
           "enforcing": {"norm_set": 6, "strategy": "full_reward_shaping", "phase": "after_training", "enforcing_horizon": 4},
           },

    # A* to test RL-params
    # levels without norms, just structure: 3x3_A, 4x4_A, 6x4_A, 6x4_B, 7x4_A, 7x4_C, 8x8_A
    "A1": {"repetitions": 100, "episodes": episodes_3x3, "max_steps": max_steps_3x3, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake3x3_A", "traverser_path": "3x3_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "distance", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "no_planning", "planning_horizon": None, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": None},
           "enforcing": None,
           },

    "A2": {"repetitions": 100, "episodes": episodes_4x4, "max_steps": max_steps_4x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake4x4_A", "traverser_path": "4x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "no_planning", "planning_horizon": None, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": None},
           "enforcing": None,
           },

    "A3": {"repetitions": 100, "episodes": episodes_4x4, "max_steps": max_steps_4x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake4x4_A", "traverser_path": "4x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "distance", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "no_planning", "planning_horizon": None, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": None},
           "enforcing": None,
           },

    "A4": {"repetitions": 100, "episodes": episodes_6x4, "max_steps": max_steps_6x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake6x4_A", "traverser_path": "6x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "no_planning", "planning_horizon": None, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": None},
           "enforcing": None,
           },

    "A5": {"repetitions": 100, "episodes": episodes_6x4, "max_steps": max_steps_6x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake6x4_B", "traverser_path": "6x4_B", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "distance", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "no_planning", "planning_horizon": None, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": None},
           "enforcing": None,
           },

    "A6": {"repetitions": 100, "episodes": episodes_7x4, "max_steps": max_steps_7x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake7x4_A", "traverser_path": "7x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "no_planning", "planning_horizon": None, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": None},
           "enforcing": None,
           },

    "A7": {"repetitions": 100, "episodes": episodes_7x4, "max_steps": max_steps_7x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake7x4_A", "traverser_path": "7x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "no_planning", "planning_horizon": None, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": None},
           "enforcing": None,
           },

    "A8": {"repetitions": 100, "episodes": episodes_7x4, "max_steps": max_steps_7x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake7x4_C", "traverser_path": "7x4_C", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "no_planning", "planning_horizon": None, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": None},
           "enforcing": None,
           },

    "A9": {"repetitions": 100, "episodes": episodes_8x8, "max_steps": max_steps_8x8, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake8x8_A", "traverser_path": "8x8_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "no_planning", "planning_horizon": None, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": None},
           "enforcing": None,
           },
    "A10": {"repetitions": 100, "episodes": episodes_8x8, "max_steps": max_steps_8x8, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake8x8_A", "traverser_path": "8x8_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "no_planning", "planning_horizon": None, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": None},
           "enforcing": None,
           },

    # TODO: higher than A7 update results in paper

    # TODO: Notes from bayesian_A
    #  tested were: episodes, dicsount, reverse-Q, learning rates (strats + decays) and init strats for 4x4_A, 6x4_A, 8x8_A
    #  reverse-Q dominated
    #  episdes were usually higher but not always the top, are used as estimates
    #  discount + learning rate had no significant impact in the tested range, hence both were set to constant in all experiments
    #  this is due to the single reward from the Frozenlake, learning rate put to 'good' value discount to 0.99
    #  init-strats are level dependant, 4-distance, 6-safe (but this still failed to work), 8-safe


    # TODO: Notes from A*
    #  A1 stabilized directly
    #  A2 failed to learn
    #  A3 learned a path -> benefit of initialization
    #  A4 + A5 failed completely like no retrn at all -> 6x4_A / _B are hard levels
    #  A6 + A7 failed to learn, the traverser in A7 was avoided by accident
    #  A8 learned a path due to safe init and avoided the traverser successfully
    #  A9 'semi'-learned a path that did not avoid the traverser
    #  Sometimes the learned path in the heat map would work on average but the policies failed to stabilized on that
    #  ..............................


    # B* to test policy strategies
    # "planning": {"norm_set": 1, "delta": 0.x, "strategy": "delta_greedy_planning", "planning_horizon": x, "reward_set": 2},
    "B1_newstates": {"repetitions": 100, "episodes": episodes_3x3, "max_steps": max_steps_3x3, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake3x3_A", "traverser_path": "3x3_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "distance", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_3x3, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },

    "B2_greedy": {"repetitions": 100, "episodes": episodes_4x4, "max_steps": max_steps_4x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake4x4_A", "traverser_path": "4x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": 0.5, "strategy": "delta_greedy_planning", "planning_horizon": ph_4x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B2_decay": {"repetitions": 100, "episodes": episodes_4x4, "max_steps": max_steps_4x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake4x4_A", "traverser_path": "4x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": 0.000005, "strategy": "delta_decaying_planning", "planning_horizon": ph_4x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B2_newstates": {"repetitions": 100, "episodes": episodes_4x4, "max_steps": max_steps_4x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake4x4_A", "traverser_path": "4x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_4x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B2_full": {"repetitions": 100, "episodes": episodes_4x4, "max_steps": max_steps_4x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake4x4_A", "traverser_path": "4x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "full_planning", "planning_horizon": ph_4x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },

    "B3_greedy": {"repetitions": 100, "episodes": episodes_4x4, "max_steps": max_steps_4x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake4x4_A", "traverser_path": "4x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "distance", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": 0.5, "strategy": "delta_greedy_planning", "planning_horizon": ph_4x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B3_decay": {"repetitions": 100, "episodes": episodes_4x4, "max_steps": max_steps_4x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake4x4_A", "traverser_path": "4x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "distance", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": 0.000005, "strategy": "delta_decaying_planning", "planning_horizon": ph_4x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B3_newstates": {"repetitions": 100, "episodes": episodes_4x4, "max_steps": max_steps_4x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake4x4_A", "traverser_path": "4x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "distance", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_4x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B3_full": {"repetitions": 100, "episodes": episodes_4x4, "max_steps": max_steps_4x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake4x4_A", "traverser_path": "4x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "distance", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "full_planning", "planning_horizon": ph_4x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },

    "B4_greedy": {"repetitions": 100, "episodes": episodes_6x4, "max_steps": max_steps_6x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake6x4_A", "traverser_path": "6x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": 0.5, "strategy": "delta_greedy_planning", "planning_horizon": ph_6x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B4_decay": {"repetitions": 100, "episodes": episodes_6x4, "max_steps": max_steps_6x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake6x4_A", "traverser_path": "6x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": 0.000005, "strategy": "delta_decaying_planning", "planning_horizon": ph_6x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B4_newstates": {"repetitions": 100, "episodes": episodes_6x4, "max_steps": max_steps_6x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake6x4_A", "traverser_path": "6x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_6x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B4_full": {"repetitions": 100, "episodes": episodes_6x4, "max_steps": max_steps_6x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake6x4_A", "traverser_path": "6x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "full_planning", "planning_horizon": ph_6x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },

    "B5_greedy": {"repetitions": 100, "episodes": episodes_6x4, "max_steps": max_steps_6x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake6x4_B", "traverser_path": "6x4_B", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "distance", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": 0.5, "strategy": "delta_greedy_planning", "planning_horizon": ph_6x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B5_decay": {"repetitions": 100, "episodes": episodes_6x4, "max_steps": max_steps_6x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake6x4_B", "traverser_path": "6x4_B", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "distance", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": 0.000005, "strategy": "delta_decaying_planning", "planning_horizon": ph_6x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B5_newstates": {"repetitions": 100, "episodes": episodes_6x4, "max_steps": max_steps_6x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake6x4_B", "traverser_path": "6x4_B", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "distance", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_6x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B5_full": {"repetitions": 100, "episodes": episodes_6x4, "max_steps": max_steps_6x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake6x4_B", "traverser_path": "6x4_B", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "distance", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "full_planning", "planning_horizon": ph_6x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },

    "B6_greedy": {"repetitions": 100, "episodes": episodes_7x4, "max_steps": max_steps_7x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake7x4_A", "traverser_path": "7x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": 0.5, "strategy": "delta_greedy_planning", "planning_horizon": ph_7x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B6_decay": {"repetitions": 100, "episodes": episodes_7x4, "max_steps": max_steps_7x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake7x4_A", "traverser_path": "7x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": 0.000005, "strategy": "delta_decaying_planning", "planning_horizon": ph_7x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B6_newstates": {"repetitions": 100, "episodes": episodes_7x4, "max_steps": max_steps_7x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake7x4_A", "traverser_path": "7x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_7x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B6_full": {"repetitions": 100, "episodes": episodes_7x4, "max_steps": max_steps_7x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake7x4_A", "traverser_path": "7x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "full_planning", "planning_horizon": ph_7x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },

    "B7_greedy": {"repetitions": 100, "episodes": episodes_7x4, "max_steps": max_steps_7x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake7x4_C", "traverser_path": "7x4_C", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": 0.5, "strategy": "delta_greedy_planning", "planning_horizon": ph_7x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B7_decay": {"repetitions": 100, "episodes": episodes_7x4, "max_steps": max_steps_7x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake7x4_C", "traverser_path": "7x4_C", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": 0.000005, "strategy": "delta_decaying_planning", "planning_horizon": ph_7x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B7_newstates": {"repetitions": 100, "episodes": episodes_7x4, "max_steps": max_steps_7x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake7x4_C", "traverser_path": "7x4_C", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_7x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B7_full": {"repetitions": 100, "episodes": episodes_7x4, "max_steps": max_steps_7x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake7x4_C", "traverser_path": "7x4_C", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "full_planning", "planning_horizon": ph_7x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },

    "B8_greedy": {"repetitions": 100, "episodes": episodes_8x8, "max_steps": max_steps_8x8, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake8x8_A", "traverser_path": "8x8_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": 0.5, "strategy": "delta_greedy_planning", "planning_horizon": ph_8x8, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B8_decay": {"repetitions": 100, "episodes": episodes_8x8, "max_steps": max_steps_8x8, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake8x8_A", "traverser_path": "8x8_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": 0.000005, "strategy": "delta_decaying_planning", "planning_horizon": ph_8x8, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B8_newstates": {"repetitions": 100, "episodes": episodes_8x8, "max_steps": max_steps_8x8, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake8x8_A", "traverser_path": "8x8_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_8x8, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B8_full": {"repetitions": 100, "episodes": episodes_8x8, "max_steps": max_steps_8x8, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake8x8_A", "traverser_path": "8x8_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "full_planning", "planning_horizon": ph_8x8, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },

    "B9_greedy": {"repetitions": 100, "episodes": episodes_8x8, "max_steps": max_steps_8x8, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake8x8_A", "traverser_path": "8x8_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": 0.5, "strategy": "delta_greedy_planning", "planning_horizon": ph_8x8, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B9_decay": {"repetitions": 100, "episodes": episodes_8x8, "max_steps": max_steps_8x8, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake8x8_A", "traverser_path": "8x8_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": 0.000005, "strategy": "delta_decaying_planning", "planning_horizon": ph_8x8, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B9_newstates": {"repetitions": 100, "episodes": episodes_8x8, "max_steps": max_steps_8x8, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake8x8_A", "traverser_path": "8x8_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_8x8, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "B9_full": {"repetitions": 100, "episodes": episodes_8x8, "max_steps": max_steps_8x8, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake8x8_A", "traverser_path": "8x8_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "full_planning", "planning_horizon": ph_8x8, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },

    # TODO: maybe redo B9 with the new time-limit, also all B's, since C's are upto date

    # TODO: Notes from B*
    #  results for high early planning (full,new,decay) ignore the initialization
    #  all plannings outperformed no_plannings, and learned the shortest path to goal
    #  higher planning yields better performance
    #  exploration underperformed, maybe epsilon was too low, or needs to be different to the planning strat? could also argue that concret epsilon strategies would help
    #  maybe for the decay use exploration later on? If so then the target performance decreases afterwards -> thus decaying planning is not beneficial
    #  for B6, the planning component aimed for the shortest path, but after the traverser blocks it, it redirects the agent, this extra step is also learned into the target policy
    #  B7 the right path was learned, but the rewards are low, due to narrow path and risk of sliding (An edge kissing was not learned, since the model has not integrated this)
    #  B7 was the hardest level (7x4_C)
    #  for B2 (and maybe some others) return of newstates is better than for full (ie. they are similar and randomness decides superior?)
    #  for B2 (and maybe some others) if the initialization does not solve the level then it hinder final return
    #  .......


    # C* to test norms and CTD
    # TODO: here planning strategy=greedy;plan_new_states;full_planning?
    #  but only... some levels should have all planning strategies and later on maybe just planning for new states?
    # TODO: deontic aspects should be simple norms and then with ctds, also some larger ones with different priorities
    # TODO: foreach config here define hypotheses

    # C1 on 3x3_B tests simple CTD, presents and then combination # TODO: re-run
    #   works as expected, Present favors present, CTD the other way, CTD and CTD with presents have equal results
    "C1_traverser": {"repetitions": 100, "episodes": episodes_3x3, "max_steps": max_steps_3x3, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake3x3_B", "traverser_path": "3x3_B", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "distance", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 2, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_3x3, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "C1_present": {"repetitions": 100, "episodes": episodes_3x3, "max_steps": max_steps_3x3, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake3x3_B", "traverser_path": "3x3_B", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "distance", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 3, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_3x3, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "C1_traverser_with_present": {"repetitions": 100, "episodes": episodes_3x3, "max_steps": max_steps_3x3, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake3x3_B", "traverser_path": "3x3_B", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "distance", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 4, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_3x3, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },

    # C2 on 4x4_B tests traverser avoidance challenged with safe area
    "C2_normless": {"repetitions": 100, "episodes": episodes_4x4, "max_steps": max_steps_4x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake4x4_B", "traverser_path": "4x4_B", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_4x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "C2_traverser": {"repetitions": 100, "episodes": episodes_4x4, "max_steps": max_steps_4x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake4x4_B", "traverser_path": "4x4_B", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 2, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_4x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "C2_safe": {"repetitions": 100, "episodes": episodes_4x4, "max_steps": max_steps_4x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake4x4_B", "traverser_path": "4x4_B", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 5, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_4x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "C2_traverser_with_safe": {"repetitions": 100, "episodes": episodes_4x4, "max_steps": max_steps_4x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake4x4_B", "traverser_path": "4x4_B", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 6, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_4x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },

    # C3 on 4x4_C tests traverser with present and different evaluation-strats (like only rewards, scaling, only violations and lastly weak constraints)
    "C3_rewards": {"repetitions": 100, "episodes": episodes_4x4, "max_steps": max_steps_4x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake4x4_C", "traverser_path": "4x4_C", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 4, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_4x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 1},
           "enforcing": None,
           },
    "C3_scaling": {"repetitions": 100, "episodes": episodes_4x4, "max_steps": max_steps_4x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake4x4_C", "traverser_path": "4x4_C", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 4, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_4x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 2},
           "enforcing": None,
           },
    "C3_violations": {"repetitions": 100, "episodes": episodes_4x4, "max_steps": max_steps_4x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake4x4_C", "traverser_path": "4x4_C", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 4, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_4x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 3},
           "enforcing": None,
           },
    "C3_weakconstrains": {"repetitions": 100, "episodes": episodes_4x4, "max_steps": max_steps_4x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake4x4_C", "traverser_path": "4x4_C", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 4, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_4x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },

    # C4 on 6x4_B tests norm of reaching vs detour to avoid traverser
    "C4_traverser": {"repetitions": 100, "episodes": episodes_6x4, "max_steps": max_steps_6x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake6x4_B", "traverser_path": "6x4_B", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 2, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_6x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "C4_moving": {"repetitions": 100, "episodes": episodes_6x4, "max_steps": max_steps_6x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake6x4_B", "traverser_path": "6x4_B", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_6x4, "reward_set": 2},
           "deontic": {"norm_set": 7, "evaluation_function": 4},
           "enforcing": None,
           },
    "C4_strictly_moving": {"repetitions": 100, "episodes": episodes_6x4, "max_steps": max_steps_6x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake6x4_B", "traverser_path": "6x4_B", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_6x4, "reward_set": 2},
           "deontic": {"norm_set": 8, "evaluation_function": 4},
           "enforcing": None,
           },
    "C4_traverser_with_moving": {"repetitions": 100, "episodes": episodes_6x4, "max_steps": max_steps_6x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake6x4_B", "traverser_path": "6x4_B", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_6x4, "reward_set": 2},
           "deontic": {"norm_set": 9, "evaluation_function": 4},
           "enforcing": None,
           },

    # C5 on 7x4_A tests safe areas vs moving to goal
    "C5_safe": {"repetitions": 100, "episodes": episodes_7x4, "max_steps": max_steps_7x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake7x4_A", "traverser_path": "7x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 5, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_7x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "C5_moving": {"repetitions": 100, "episodes": episodes_7x4, "max_steps": max_steps_7x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake7x4_A", "traverser_path": "7x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 7, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_7x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "C5_safe_first": {"repetitions": 100, "episodes": episodes_7x4, "max_steps": max_steps_7x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake7x4_A", "traverser_path": "7x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 10, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_7x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "C5_moving_first": {"repetitions": 100, "episodes": episodes_7x4, "max_steps": max_steps_7x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake7x4_A", "traverser_path": "7x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 11, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_7x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "C5_equal": {"repetitions": 100, "episodes": episodes_7x4, "max_steps": max_steps_7x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake7x4_A", "traverser_path": "7x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 12, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_7x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },

    # C6 on 7x4_B tests the safe areas but with traverser
    "C6_traverser_first": {"repetitions": 100, "episodes": episodes_7x4, "max_steps": max_steps_7x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake7x4_B", "traverser_path": "7x4_B", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 13, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_7x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "C6_safe_first": {"repetitions": 100, "episodes": episodes_7x4, "max_steps": max_steps_7x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake7x4_B", "traverser_path": "7x4_B", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 14, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_7x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "C6_equal": {"repetitions": 100, "episodes": episodes_7x4, "max_steps": max_steps_7x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake7x4_B", "traverser_path": "7x4_B", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 15, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_7x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },

    # C7 on 7x4_C tests  moving towards goal, can make a failure here
    "C7_normless": {"repetitions": 100, "episodes": episodes_7x4, "max_steps": max_steps_7x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake7x4_C", "traverser_path": "7x4_C", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_7x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "C7_moving": {"repetitions": 100, "episodes": episodes_7x4, "max_steps": max_steps_7x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake7x4_C", "traverser_path": "7x4_C", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 7, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_7x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "C7_strictly_moving": {"repetitions": 100, "episodes": episodes_7x4, "max_steps": max_steps_7x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake7x4_C", "traverser_path": "7x4_C", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 8, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_7x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "C7_traverser_with_moving": {"repetitions": 100, "episodes": episodes_7x4, "max_steps": max_steps_7x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake7x4_C", "traverser_path": "7x4_C", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": "todo", "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_7x4, "reward_set": 2},
           "deontic": {"norm_set": 9, "evaluation_function": 4},
           "enforcing": None,
           },

    # C8 on 7x4_D tests presents vs moving towards goal (maybe also some evaluations here)
    "C8_normless": {"repetitions": 100, "episodes": episodes_7x4, "max_steps": max_steps_7x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake7x4_D", "traverser_path": "7x4_D", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 1, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_7x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "C8_no_presents": {"repetitions": 100, "episodes": episodes_7x4, "max_steps": max_steps_7x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake7x4_D", "traverser_path": "7x4_D", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 16, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_7x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "C8_presents": {"repetitions": 100, "episodes": episodes_7x4, "max_steps": max_steps_7x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake7x4_D", "traverser_path": "7x4_D", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 3, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_7x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "C8_moving": {"repetitions": 100, "episodes": episodes_7x4, "max_steps": max_steps_7x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake7x4_D", "traverser_path": "7x4_D", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 7, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_7x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },

    # C9 on 8x8_A tests just traverser (comp with the one with B*)
    "C9_traverser": {"repetitions": 100, "episodes": episodes_8x8, "max_steps": max_steps_8x8, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake8x8_A", "traverser_path": "8x8_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 2, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_8x8, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "C9_traverser_over_safe": {"repetitions": 100, "episodes": episodes_8x8, "max_steps": max_steps_8x8, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake8x8_A", "traverser_path": "8x8_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 13, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_8x8, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },

    # C10 on 8x8_B tests presents vs reward vs moving towards goal
    # "C10_normless": {"repetitions": 100, "episodes": episodes_8x8, "max_steps": max_steps_8x8, "evaluation_repetitions": 100,
    #        "frozenlake": {"name": "FrozenLake8x8_B", "traverser_path": "8x8_B", "slippery": True},
    #        "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
    #        "planning": {"norm_set": 1, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_8x8, "reward_set": 2},
    #        "deontic": {"norm_set": 0, "evaluation_function": 4},
    #        "enforcing": None,
    #        },
    "C10_safe": {"repetitions": 100, "episodes": episodes_8x8, "max_steps": max_steps_8x8, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake8x8_B", "traverser_path": "8x8_B", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 5, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_8x8, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "C10_presents": {"repetitions": 100, "episodes": episodes_8x8, "max_steps": max_steps_8x8, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake8x8_B", "traverser_path": "8x8_B", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 3, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_8x8, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    # TODO: re-run all experiments with presents
    # TODO: run both experiments below
    "C10_safe_with_presents_1": {"repetitions": 100, "episodes": episodes_8x8, "max_steps": max_steps_8x8, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake8x8_B", "traverser_path": "8x8_B", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 17, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_8x8, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "C10_safe_with_presents_2": {"repetitions": 100, "episodes": episodes_8x8, "max_steps": max_steps_8x8, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake8x8_B", "traverser_path": "8x8_B", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "safe", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 18, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_8x8, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },

    # TODO: Notes from C*
    #  .
    #  The plan_for_new_states some issues when sliding, because the planning chain then gets broken, need to find an example to illustrate this, or have a C11 with full planning?
    #  .
    #  ...


    # D* to test dilemmas + alternatives in deontic reasoning + 'failures' or pitfalls
    # TODO: different rewards,
    #  different evaluations?,
    #  worse plannings + play around without reaching goal norm,
    #  normative pitfalls in successfully reaching goal/ conflicts (could correlate to some paradoxes),
    #  forbidden -> not permitted, obligatory -> forbidden(not permitted),
    #  deontic vs factual detachment,
    #  paradoxes-simulations

    # TODO: safe init is not the same as safe_area -> check and fix in paper


    # E* to test enforcings
    # "enforcing": {"norm_set": 6, "strategy": "guardrail | fixing | optimal_reward_shaping | full_reward_shaping", "phase": "during_training | after_training", "enforcing_horizon": [3,6] (no use in guardral; in fixing is list [len of checked path; len of fixed path]; in reward-shaping defines number of shaping steps)},
    # TODO: a guardrail with a simple reaching goal norm could help even without planning, the rewards shaping could also use the dependency on planning

    # On 4x4_B during training for traverser
    "E1_traverser_guard": {"repetitions": 100, "episodes": episodes_4x4, "max_steps": max_steps_4x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake4x4_B", "traverser_path": "4x4_B", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 2, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_4x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": {"norm_set": 2, "strategy": "guardrail", "phase": "during_training", "enforcing_horizon": None},
           },
    "E1_traverser_fix": {"repetitions": 100, "episodes": episodes_4x4, "max_steps": max_steps_4x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake4x4_B", "traverser_path": "4x4_B", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 2, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_4x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": {"norm_set": 2, "strategy": "fixing", "phase": "during_training", "enforcing_horizon": [4, ph_4x4]},
           },
    "E1_traverser_full_shaping": {"repetitions": 100, "episodes": episodes_4x4, "max_steps": max_steps_4x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake4x4_B", "traverser_path": "4x4_B", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 2, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_4x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": {"norm_set": 2, "strategy": "full_reward_shaping", "phase": "during_training", "enforcing_horizon": [100]},
           },

    # On 4x4_B after training for traverser
    "E2_traverser_guard": {"repetitions": 100, "episodes": episodes_4x4, "max_steps": max_steps_4x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake4x4_B", "traverser_path": "4x4_B", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 2, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_4x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": {"norm_set": 2, "strategy": "guardrail", "phase": "after_training", "enforcing_horizon": None},
           },
    "E2_traverser_fix": {"repetitions": 100, "episodes": episodes_4x4, "max_steps": max_steps_4x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake4x4_B", "traverser_path": "4x4_B", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 2, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_4x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": {"norm_set": 2, "strategy": "fixing", "phase": "after_training", "enforcing_horizon": [4, ph_4x4]},
           },
    "E2_traverser_full_shaping": {"repetitions": 100, "episodes": episodes_4x4, "max_steps": max_steps_4x4, "evaluation_repetitions": 100,
           "frozenlake": {"name": "FrozenLake4x4_B", "traverser_path": "4x4_B", "slippery": True},
           "learning": {"norm_set": None, "epsilon": epsilon, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": None},
           "planning": {"norm_set": 2, "delta": None, "strategy": "plan_for_new_states", "planning_horizon": ph_4x4, "reward_set": 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": {"norm_set": 2, "strategy": "full_reward_shaping", "phase": "after_training", "enforcing_horizon": [100]},
           },


    # TODO: test and debug the other enforcings, update cache for validating (different cache for that?), update violation plot and the tests




    "U4_1": {"repetitions": 20, "episodes": 60, "max_steps": 20, "evaluation_repetitions": 20,
           "frozenlake": {"name": "FrozenLake4x4_B", "traverser_path": "4x4_B", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.3, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": 0.02},
           "planning": {"norm_set": 8, "delta": 0.5, "strategy": "delta_greedy_planning", "planning_horizon": 9, "reward_set" : 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": {"norm_set": 10, "strategy": "guardrail", "phase": "after_training", "enforcing_horizon": None},
           },
    "U6_1": {"repetitions": 20, "episodes": 100, "max_steps": 30,
           "frozenlake": {"name": "FrozenLake6x4_A", "traverser_path": "6x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.3, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": 0.02},
           "planning": {"norm_set": 9, "delta": 0.75, "strategy": "delta_greedy_planning", "planning_horizon": 14, "reward_set" : 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": {"norm_set": 11, "strategy": "guardrail", "phase": "after_training", "enforcing_horizon": None},
           },

    "U4_2": {"repetitions": 20, "episodes": 60, "max_steps": 20, "evaluation_repetitions": 20,
           "frozenlake": {"name": "FrozenLake4x4_B", "traverser_path": "4x4_B", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.3, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": 0.02},
           "planning": {"norm_set": 8, "delta": 0.5, "strategy": "delta_greedy_planning", "planning_horizon": 9, "reward_set" : 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": {"norm_set": 10, "strategy": "fixing", "phase": "after_training", "enforcing_horizon": [4,9]},
           },
    "U6_2": {"repetitions": 20, "episodes": 100, "max_steps": 30,
           "frozenlake": {"name": "FrozenLake6x4_A", "traverser_path": "6x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.3, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": 0.02},
           "planning": {"norm_set": 9, "delta": 0.75, "strategy": "delta_greedy_planning", "planning_horizon": 14, "reward_set" : 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": {"norm_set": 11, "strategy": "fixing", "phase": "after_training", "enforcing_horizon": [7,14]},
           },

    "U4_3": {"repetitions": 20, "episodes": 60, "max_steps": 20, "evaluation_repetitions": 20,
           "frozenlake": {"name": "FrozenLake4x4_B", "traverser_path": "4x4_B", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.3, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": 0.02},
           "planning": {"norm_set": 8, "delta": 0.5, "strategy": "delta_greedy_planning", "planning_horizon": 9, "reward_set" : 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": {"norm_set": 10, "strategy": "optimal_reward_shaping", "phase": "after_training", "enforcing_horizon": [60]},
           },
    "U6_3": {"repetitions": 20, "episodes": 100, "max_steps": 30,
           "frozenlake": {"name": "FrozenLake6x4_A", "traverser_path": "6x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.3, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": 0.02},
           "planning": {"norm_set": 9, "delta": 0.75, "strategy": "delta_greedy_planning", "planning_horizon": 14, "reward_set" : 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": {"norm_set": 11, "strategy": "optimal_reward_shaping", "phase": "after_training", "enforcing_horizon": [75]},
           },


    "U4_4": {"repetitions": 20, "episodes": 60, "max_steps": 20, "evaluation_repetitions": 20,
           "frozenlake": {"name": "FrozenLake4x4_B", "traverser_path": "4x4_B", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.3, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": 0.02},
           "planning": {"norm_set": 8, "delta": 0.5, "strategy": "delta_greedy_planning", "planning_horizon": 9, "reward_set" : 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": {"norm_set": 10, "strategy": "full_reward_shaping", "phase": "after_training", "enforcing_horizon": [60]},
           },
    "U6_4": {"repetitions": 20, "episodes": 100, "max_steps": 30,
           "frozenlake": {"name": "FrozenLake6x4_A", "traverser_path": "6x4_A", "slippery": True},
           "learning": {"norm_set": None, "epsilon": 0.3, "initialization": "zero", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": 0.02},
           "planning": {"norm_set": 9, "delta": 0.75, "strategy": "delta_greedy_planning", "planning_horizon": 14, "reward_set" : 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": {"norm_set": 11, "strategy": "full_reward_shaping", "phase": "after_training", "enforcing_horizon": [75]},
           },

    "U4_5": {"repetitions": 20, "episodes": 60, "max_steps": 20, "evaluation_repetitions": 20,
           "frozenlake": {"name": "FrozenLake4x4_B", "traverser_path": "4x4_B", "slippery": True},
           "learning": {"norm_set": 10, "epsilon": 0.3, "initialization": "state_function", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": 0.02},
           "planning": {"norm_set": 8, "delta": 0.5, "strategy": "delta_greedy_planning", "planning_horizon": 9, "reward_set" : 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "U6_5": {"repetitions": 20, "episodes": 100, "max_steps": 30,
           "frozenlake": {"name": "FrozenLake6x4_A", "traverser_path": "6x4_A", "slippery": True},
           "learning": {"norm_set": 11, "epsilon": 0.3, "initialization": "state_function", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": 0.02},
           "planning": {"norm_set": 9, "delta": 0.75, "strategy": "delta_greedy_planning", "planning_horizon": 14, "reward_set" : 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },

    "U4_6": {"repetitions": 20, "episodes": 60, "max_steps": 20, "evaluation_repetitions": 20,
           "frozenlake": {"name": "FrozenLake4x4_B", "traverser_path": "4x4_B", "slippery": True},
           "learning": {"norm_set": 10, "epsilon": 0.3, "initialization": "state_action_penalty", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": 0.02},
           "planning": {"norm_set": 8, "delta": 0.5, "strategy": "delta_greedy_planning", "planning_horizon": 9, "reward_set" : 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "U6_6": {"repetitions": 20, "episodes": 100, "max_steps": 30,
           "frozenlake": {"name": "FrozenLake6x4_A", "traverser_path": "6x4_A", "slippery": True},
           "learning": {"norm_set": 11, "epsilon": 0.3, "initialization": "state_action_penalty", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": 0.02},
           "planning": {"norm_set": 9, "delta": 0.75, "strategy": "delta_greedy_planning", "planning_horizon": 14, "reward_set" : 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "U4_7": {"repetitions": 20, "episodes": 60, "max_steps": 20, "evaluation_repetitions": 20,
           "frozenlake": {"name": "FrozenLake4x4_B", "traverser_path": "4x4_B", "slippery": True},
           "learning": {"norm_set": 10, "epsilon": 0.3, "initialization": "distance", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": 0.02},
           "planning": {"norm_set": 8, "delta": 0.5, "strategy": "delta_greedy_planning", "planning_horizon": 9, "reward_set" : 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
    "U6_7": {"repetitions": 20, "episodes": 100, "max_steps": 30,
           "frozenlake": {"name": "FrozenLake6x4_A", "traverser_path": "6x4_A", "slippery": True},
           "learning": {"norm_set": 11, "epsilon": 0.3, "initialization": "distance", "reversed_q_learning": True, "discount": 0.99, "learning_rate": 0.3, "learning_rate_strategy": "constant", "learning_decay_rate": 0.02},
           "planning": {"norm_set": 9, "delta": 0.75, "strategy": "delta_greedy_planning", "planning_horizon": 14, "reward_set" : 2},
           "deontic": {"norm_set": 0, "evaluation_function": 4},
           "enforcing": None,
           },
}